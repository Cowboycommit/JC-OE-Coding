"""
LLM-assisted cluster interpretation for improved human readability.

This module provides optional LLM-based refinement of cluster labels and descriptions,
using Mistral API with silent fallback to local quantized models.

Key features:
- Post-clustering label refinement using LLM
- Generation of alternative topic labels
- Detailed topic descriptions beyond simple labels
- Silent fallback to local quantized Mistral models if API fails
- Fully optional - graceful degradation to term-based labels

Configuration via environment variables:
    MISTRAL_API_KEY: API key for Mistral API (optional)
    MISTRAL_DEFAULT_MODEL: Model to use (default: open-mixtral-8x7b)
    LLM_INTERPRETATION_ENABLED: Enable/disable LLM interpretation (default: true)
    LLM_LOCAL_MODEL_PATH: Path to local quantized model (default: models/)

Usage:
    >>> from src.llm_interpretation import LLMClusterInterpreter
    >>> interpreter = LLMClusterInterpreter()
    >>> enhanced_summary = interpreter.enhance_cluster_summary(cluster_summary)
"""

import os
import json
import logging
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

logger = logging.getLogger(__name__)

# Try to load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    logger.debug("python-dotenv not installed, using system environment variables only")


@dataclass
class LLMEnhancedLabel:
    """
    Enhanced cluster label generated by LLM.

    Attributes:
        primary_label: The main refined label for the cluster
        alternative_labels: List of alternative label suggestions
        description: Detailed description of what the cluster represents
        reasoning: Brief explanation of why this label was chosen
        confidence: LLM's confidence in the interpretation (0-1)
        source: Whether generated by 'api', 'local', or 'fallback' (term-based)
    """
    primary_label: str
    alternative_labels: List[str] = field(default_factory=list)
    description: str = ""
    reasoning: str = ""
    confidence: float = 0.0
    source: str = "fallback"

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            'primary_label': self.primary_label,
            'alternative_labels': self.alternative_labels,
            'description': self.description,
            'reasoning': self.reasoning,
            'confidence': round(self.confidence, 3),
            'source': self.source
        }


class MistralAPIClient:
    """
    Client for Mistral API with error handling.
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        timeout: int = 30,
        temperature: float = 0.1,
        max_retries: int = 3
    ):
        """
        Initialize Mistral API client.

        Args:
            api_key: Mistral API key (defaults to MISTRAL_API_KEY env var)
            model: Model to use (defaults to MISTRAL_DEFAULT_MODEL env var)
            timeout: Request timeout in seconds
            temperature: Sampling temperature (lower = more consistent)
            max_retries: Maximum number of retry attempts on failure
        """
        self.api_key = api_key or os.getenv("MISTRAL_API_KEY")
        self.model = model or os.getenv("MISTRAL_DEFAULT_MODEL", "open-mixtral-8x7b")
        self.timeout = timeout
        self.temperature = temperature
        self.max_retries = max_retries
        self._client = None
        self._available = None

    def _init_client(self) -> bool:
        """Initialize the Mistral client lazily."""
        if self._available is not None:
            return self._available

        if not self.api_key:
            logger.info("No MISTRAL_API_KEY found, API client not available")
            self._available = False
            return False

        try:
            from mistralai import Mistral
            self._client = Mistral(
                api_key=self.api_key,
                timeout_ms=self.timeout * 1000  # Convert seconds to milliseconds
            )
            self._available = True
            logger.info(f"Mistral API client initialized with model: {self.model}")
            return True
        except ImportError:
            logger.warning("mistralai package not installed, API client not available")
            self._available = False
            return False
        except Exception as e:
            logger.warning(f"Failed to initialize Mistral client: {e}")
            self._available = False
            return False

    @property
    def is_available(self) -> bool:
        """Check if the API client is available."""
        return self._init_client()

    def generate(self, prompt: str, system_prompt: Optional[str] = None) -> Optional[str]:
        """
        Generate a response from the Mistral API with retry logic.

        Args:
            prompt: User prompt
            system_prompt: Optional system prompt

        Returns:
            Generated text or None if failed
        """
        if not self._init_client():
            return None

        import time

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        last_error = None
        for attempt in range(self.max_retries):
            try:
                logger.debug(f"Mistral API call attempt {attempt + 1}/{self.max_retries} with model: {self.model}")
                response = self._client.chat.complete(
                    model=self.model,
                    messages=messages,
                    temperature=self.temperature,
                    stream=False
                )

                if response and response.choices:
                    content = response.choices[0].message.content
                    logger.debug(f"Mistral API call successful, response length: {len(content) if content else 0}")
                    return content
                logger.warning("Mistral API returned empty response or no choices")
                return None

            except Exception as e:
                last_error = e
                if attempt < self.max_retries - 1:
                    wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s
                    logger.warning(f"Mistral API call failed (attempt {attempt + 1}), retrying in {wait_time}s: {e}")
                    time.sleep(wait_time)

        logger.error(f"Mistral API call failed after {self.max_retries} attempts: {last_error}")
        return None


class LocalMistralClient:
    """
    Client for local quantized Mistral models using llama-cpp-python.

    Supports GGUF format models for efficient CPU/GPU inference.
    """

    # Default model filenames to search for
    DEFAULT_MODEL_NAMES = [
        "mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf",
        "mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf",
        "mistral-7b-instruct-v0.2.Q4_K_M.gguf",
        "mistral-7b-instruct-v0.2.Q5_K_M.gguf",
        "open-mistral-7b.Q4_K_M.gguf",
        "open-mistral-7b.Q5_K_M.gguf",
    ]

    def __init__(
        self,
        model_path: Optional[str] = None,
        n_ctx: int = 4096,
        n_threads: Optional[int] = None,
        n_gpu_layers: int = 0
    ):
        """
        Initialize local Mistral client.

        Args:
            model_path: Path to GGUF model file or directory containing models
            n_ctx: Context window size
            n_threads: Number of CPU threads (None for auto)
            n_gpu_layers: Number of layers to offload to GPU (0 for CPU only)
        """
        self.model_path = model_path or os.getenv("LLM_LOCAL_MODEL_PATH", "models/")
        self.n_ctx = n_ctx
        self.n_threads = n_threads
        self.n_gpu_layers = n_gpu_layers
        self._model = None
        self._available = None
        self._resolved_path = None

    def _find_model_file(self) -> Optional[Path]:
        """Find a valid model file."""
        path = Path(self.model_path)

        # If it's a file, use it directly
        if path.is_file() and path.suffix == ".gguf":
            return path

        # If it's a directory, search for model files
        if path.is_dir():
            # First, check for any of the default model names
            for name in self.DEFAULT_MODEL_NAMES:
                model_file = path / name
                if model_file.exists():
                    return model_file

            # Then, look for any .gguf file
            gguf_files = list(path.glob("*.gguf"))
            if gguf_files:
                # Prefer mixtral over mistral-7b
                mixtral_files = [f for f in gguf_files if "mixtral" in f.name.lower()]
                if mixtral_files:
                    return mixtral_files[0]
                return gguf_files[0]

        return None

    def _init_model(self) -> bool:
        """Initialize the local model lazily."""
        if self._available is not None:
            return self._available

        try:
            from llama_cpp import Llama
        except ImportError:
            logger.info("llama-cpp-python not installed, local model not available")
            self._available = False
            return False

        model_file = self._find_model_file()
        if not model_file:
            logger.info(f"No GGUF model found in {self.model_path}")
            self._available = False
            return False

        try:
            self._resolved_path = model_file
            logger.info(f"Loading local model from {model_file}...")

            self._model = Llama(
                model_path=str(model_file),
                n_ctx=self.n_ctx,
                n_threads=self.n_threads,
                n_gpu_layers=self.n_gpu_layers,
                verbose=False
            )

            self._available = True
            logger.info(f"Local model loaded successfully: {model_file.name}")
            return True

        except Exception as e:
            logger.warning(f"Failed to load local model: {e}")
            self._available = False
            return False

    @property
    def is_available(self) -> bool:
        """Check if the local model is available."""
        return self._init_model()

    @property
    def model_name(self) -> str:
        """Get the name of the loaded model."""
        if self._resolved_path:
            return self._resolved_path.stem
        return "unknown"

    def generate(self, prompt: str, system_prompt: Optional[str] = None) -> Optional[str]:
        """
        Generate a response using the local model.

        Args:
            prompt: User prompt
            system_prompt: Optional system prompt

        Returns:
            Generated text or None if failed
        """
        if not self._init_model():
            return None

        try:
            # Format as Mistral instruct format
            if system_prompt:
                full_prompt = f"<s>[INST] {system_prompt}\n\n{prompt} [/INST]"
            else:
                full_prompt = f"<s>[INST] {prompt} [/INST]"

            response = self._model(
                full_prompt,
                max_tokens=1024,
                temperature=0.1,  # Lower temperature for more consistent labels
                top_p=0.9,
                stop=["</s>", "[INST]"]
            )

            if response and "choices" in response:
                return response["choices"][0]["text"].strip()
            return None

        except Exception as e:
            logger.warning(f"Local model generation failed: {e}")
            return None


class LLMClusterInterpreter:
    """
    LLM-assisted cluster interpretation with API and local fallback.

    This class enhances cluster summaries with LLM-generated labels,
    alternative suggestions, and detailed descriptions. It gracefully
    falls back to local models if API is unavailable, and to term-based
    labels if both fail.

    Example:
        >>> interpreter = LLMClusterInterpreter()
        >>> enhanced = interpreter.enhance_cluster_summary(
        ...     cluster_summary=summary,
        ...     sample_texts=["example 1", "example 2"]
        ... )
        >>> print(enhanced.primary_label)
        >>> print(enhanced.description)
    """

    # System prompt for cluster interpretation
    SYSTEM_PROMPT = """You are an expert qualitative researcher specializing in thematic analysis and coding of open-ended survey responses.

Your task is to analyze a cluster of similar survey responses and generate an accurate, specific label that captures the core theme.

CRITICAL REQUIREMENTS FOR GOOD LABELS:
1. Be SPECIFIC, not generic - avoid vague labels like "General Feedback", "Various Issues", "Multiple Topics"
2. Labels must be 2-4 words that capture the DISTINCTIVE theme of this cluster
3. Focus on WHAT people are saying, not HOW they're saying it
4. Identify the specific subject matter (e.g., "Slow Response Times" not "Service Issues")
5. If the cluster discusses a problem, name the specific problem
6. If discussing positive feedback, name what's being praised specifically

EXAMPLES OF BAD vs GOOD LABELS:
- BAD: "Customer Service" → GOOD: "Long Wait Times" or "Helpful Staff"
- BAD: "Product Issues" → GOOD: "Battery Life Problems" or "Confusing Interface"
- BAD: "General Satisfaction" → GOOD: "Easy Setup Process" or "Reliable Performance"
- BAD: "Negative Feedback" → GOOD: "Billing Errors" or "Shipping Delays"

Your output must include:
1. primary_label: A specific 2-4 word label capturing the unique theme
2. alternative_labels: REQUIRED - provide exactly 3 alternative labels that capture the theme from different angles or emphasize different aspects. These are important for giving users options.
3. description: 2-3 sentences explaining what responses in this cluster share
4. reasoning: Brief explanation of why you chose this interpretation
5. confidence: Score 0-1 based on how clear/coherent the cluster theme is

IMPORTANT: You MUST provide exactly 3 alternative_labels. Each alternative should be a valid 2-4 word label that could work as the primary label but approaches the theme differently."""

    def __init__(
        self,
        api_key: Optional[str] = None,
        api_model: Optional[str] = None,
        local_model_path: Optional[str] = None,
        enabled: Optional[bool] = None
    ):
        """
        Initialize the LLM cluster interpreter.

        Args:
            api_key: Mistral API key (defaults to env var)
            api_model: API model name (defaults to env var)
            local_model_path: Path to local model (defaults to env var)
            enabled: Whether LLM interpretation is enabled (defaults to env var)
        """
        # Check if enabled
        if enabled is None:
            enabled_str = os.getenv("LLM_INTERPRETATION_ENABLED", "true").lower()
            enabled = enabled_str in ("true", "1", "yes", "on")
        self.enabled = enabled

        if not self.enabled:
            logger.info("LLM interpretation is disabled")
            self._api_client = None
            self._local_client = None
            return

        # Initialize clients
        self._api_client = MistralAPIClient(api_key=api_key, model=api_model)
        self._local_client = LocalMistralClient(model_path=local_model_path)

        # Log availability
        if self._api_client.is_available:
            logger.info(f"LLM interpretation using API: {self._api_client.model}")
        elif self._local_client.is_available:
            logger.info(f"LLM interpretation using local model: {self._local_client.model_name}")
        else:
            logger.warning("No LLM backend available, will use term-based fallback")

    @property
    def is_available(self) -> bool:
        """Check if any LLM backend is available."""
        if not self.enabled:
            return False
        return (
            (self._api_client and self._api_client.is_available) or
            (self._local_client and self._local_client.is_available)
        )

    @property
    def backend_type(self) -> str:
        """Get the type of backend being used."""
        if not self.enabled:
            return "disabled"
        if self._api_client and self._api_client.is_available:
            return "api"
        if self._local_client and self._local_client.is_available:
            return "local"
        return "fallback"

    def _build_interpretation_prompt(
        self,
        top_terms: List[str],
        term_weights: List[float],
        sample_texts: List[str],
        current_label: str,
        document_count: int
    ) -> str:
        """Build the prompt for cluster interpretation with rich context."""
        # Format top terms with weights - include more terms for better context
        terms_with_weights = []
        for term, weight in zip(top_terms[:15], term_weights[:15]):
            terms_with_weights.append(f"  - \"{term}\" (weight: {weight:.3f})")
        terms_str = "\n".join(terms_with_weights)

        # Format sample texts - include more samples with longer text
        samples_str = ""
        if sample_texts:
            samples = []
            for i, text in enumerate(sample_texts[:12], 1):
                # Allow longer texts for better context
                truncated = text[:800] + "..." if len(text) > 800 else text
                samples.append(f"  {i}. \"{truncated}\"")
            samples_str = "\n".join(samples)

        prompt = f"""Analyze this cluster of survey responses and provide a SPECIFIC, ACCURATE label.

CLUSTER STATISTICS:
- Total responses in cluster: {document_count}
- Auto-generated keyword label: "{current_label}"

TOP KEYWORDS BY TF-IDF WEIGHT (these indicate frequent/distinctive terms):
{terms_str}

REPRESENTATIVE SAMPLE RESPONSES (read these carefully to understand the theme):
{samples_str if samples_str else "  (No samples available)"}

ANALYSIS INSTRUCTIONS:
1. Read ALL sample responses carefully - the label must fit ALL of them, not just one
2. Identify the COMMON THEME across all samples - what specific topic/issue/feedback do they share?
3. The keywords show what terms are frequent, but the samples show the actual meaning
4. Avoid generic labels - be as specific as possible about the actual content

Based on your analysis, provide your response in this exact JSON format:
{{
    "primary_label": "Specific 2-4 word label",
    "alternative_labels": ["Different Angle Label", "Another Perspective", "Third Option"],
    "description": "2-3 sentences explaining what these responses have in common and what distinguishes this cluster from others",
    "reasoning": "Brief explanation of how you determined the common theme from the samples",
    "confidence": 0.85
}}

CRITICAL REQUIREMENTS:
- Respond with ONLY the JSON object, no other text
- You MUST provide exactly 3 alternative_labels - this is required, not optional
- Each alternative should be a different but valid way to label this cluster"""

        return prompt

    # List of vague labels to avoid - if LLM returns these, we should penalize confidence
    VAGUE_LABELS = {
        "general feedback", "various issues", "multiple topics", "other responses",
        "miscellaneous", "general comments", "mixed feedback", "general concerns",
        "customer feedback", "user feedback", "general", "various", "other",
        "feedback", "comments", "issues", "concerns", "responses", "topics"
    }

    def _parse_llm_response(self, response: str) -> Optional[Dict[str, Any]]:
        """Parse the LLM response into structured data."""
        if not response:
            return None

        try:
            # Try to extract JSON from the response
            # Sometimes LLMs add extra text before/after the JSON
            response = response.strip()

            # Find JSON block
            start_idx = response.find("{")
            end_idx = response.rfind("}") + 1

            if start_idx >= 0 and end_idx > start_idx:
                json_str = response[start_idx:end_idx]
                parsed = json.loads(json_str)

                # Validate and potentially adjust the parsed response
                parsed = self._validate_and_adjust_label(parsed)
                return parsed
            return None

        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse LLM response as JSON: {e}")
            return None

    def _validate_and_adjust_label(self, parsed: Dict[str, Any]) -> Dict[str, Any]:
        """Validate the LLM-generated label and adjust confidence if it's too vague."""
        primary_label = parsed.get("primary_label", "").lower().strip()

        # Check if the label is too vague
        is_vague = (
            primary_label in self.VAGUE_LABELS or
            any(vague in primary_label for vague in self.VAGUE_LABELS if len(vague) > 5)
        )

        if is_vague:
            # Reduce confidence significantly for vague labels
            current_confidence = float(parsed.get("confidence", 0.8))
            parsed["confidence"] = min(current_confidence, 0.4)
            logger.debug(f"Vague label detected: '{parsed.get('primary_label')}' - reducing confidence")

            # If there are alternative labels, see if any are better
            alternatives = parsed.get("alternative_labels", [])
            for alt in alternatives:
                alt_lower = alt.lower().strip()
                if alt_lower not in self.VAGUE_LABELS and not any(
                    vague in alt_lower for vague in self.VAGUE_LABELS if len(vague) > 5
                ):
                    # Use the better alternative as primary
                    logger.debug(f"Swapping vague label '{parsed['primary_label']}' with better alternative '{alt}'")
                    old_primary = parsed["primary_label"]
                    parsed["primary_label"] = alt
                    parsed["alternative_labels"] = [old_primary] + [a for a in alternatives if a != alt]
                    parsed["confidence"] = min(current_confidence, 0.7)  # Still slightly lower confidence
                    break

        # Validate label length (should be 2-4 words)
        word_count = len(parsed.get("primary_label", "").split())
        if word_count < 2:
            parsed["confidence"] = min(float(parsed.get("confidence", 0.8)), 0.5)
            logger.debug(f"Label too short ({word_count} words): '{parsed.get('primary_label')}'")
        elif word_count > 5:
            parsed["confidence"] = min(float(parsed.get("confidence", 0.8)), 0.6)
            logger.debug(f"Label too long ({word_count} words): '{parsed.get('primary_label')}'")

        # Ensure alternative_labels is a list
        if not isinstance(parsed.get("alternative_labels"), list):
            parsed["alternative_labels"] = []

        # Filter out empty or invalid alternatives
        parsed["alternative_labels"] = [
            alt for alt in parsed.get("alternative_labels", [])
            if alt and isinstance(alt, str) and len(alt.strip()) > 0
        ]

        return parsed

    def _generate_alternative_from_terms(
        self,
        top_terms: List[str],
        primary_label: str,
        existing_alternatives: List[str]
    ) -> List[str]:
        """
        Generate alternative labels from top terms when LLM doesn't provide enough.

        This creates synthetic alternatives by combining top terms in different ways.
        """
        alternatives = list(existing_alternatives)  # Copy existing
        used_words = set(primary_label.lower().split())
        for alt in alternatives:
            used_words.update(alt.lower().split())

        # Try to create alternatives from top terms that aren't already used
        candidate_words = []
        for term in top_terms[:10]:
            for word in term.split():
                word_lower = word.lower().strip()
                if (word_lower not in used_words and
                    word_lower not in self.VAGUE_LABELS and
                    len(word_lower) > 2):
                    candidate_words.append(word.title())
                    used_words.add(word_lower)

        # Generate alternatives by pairing terms
        while len(alternatives) < 3 and len(candidate_words) >= 2:
            # Take pairs of words to form alternatives
            alt = f"{candidate_words[0]} {candidate_words[1]}"
            if alt.lower() != primary_label.lower() and alt not in alternatives:
                alternatives.append(alt)
            candidate_words = candidate_words[2:]  # Remove used words

        # If still not enough, try single descriptive terms with modifiers
        if len(alternatives) < 3 and candidate_words:
            for word in candidate_words[:3 - len(alternatives)]:
                alt = f"{word} Related"
                if alt not in alternatives:
                    alternatives.append(alt)

        return alternatives[:3]

    def _generate_with_fallback(self, prompt: str) -> Tuple[Optional[str], str]:
        """
        Generate response with API-first, local-fallback strategy.

        Returns:
            Tuple of (response_text, source) where source is 'api', 'local', or 'none'
        """
        # Try API first
        if self._api_client and self._api_client.is_available:
            logger.info("Attempting LLM interpretation via Mistral API...")
            response = self._api_client.generate(prompt, self.SYSTEM_PROMPT)
            if response:
                logger.info("Mistral API returned successful response for topic refinement")
                return response, "api"
            logger.warning("Mistral API call returned no response, trying local model fallback")
        else:
            logger.info("Mistral API not available, trying local model")

        # Fall back to local model
        if self._local_client and self._local_client.is_available:
            logger.info("Attempting LLM interpretation via local model...")
            response = self._local_client.generate(prompt, self.SYSTEM_PROMPT)
            if response:
                logger.info("Local model returned successful response for topic refinement")
                return response, "local"
            logger.warning("Local model returned no response")
        else:
            logger.info("Local model not available")

        logger.warning("No LLM backend available for topic refinement, using term-based fallback")
        return None, "none"

    def enhance_cluster_summary(
        self,
        top_terms: List[str],
        term_weights: List[float],
        current_label: str,
        document_count: int,
        sample_texts: Optional[List[str]] = None
    ) -> LLMEnhancedLabel:
        """
        Enhance a cluster summary with LLM-generated interpretation.

        Args:
            top_terms: List of top terms for the cluster
            term_weights: Weights for each top term
            current_label: Current auto-generated label
            document_count: Number of documents in cluster
            sample_texts: Optional sample texts from the cluster

        Returns:
            LLMEnhancedLabel with refined label and description
        """
        # Return fallback if LLM not enabled or available
        if not self.enabled or not self.is_available:
            # Still generate alternatives from terms even without LLM
            fallback_alternatives = self._generate_alternative_from_terms(
                top_terms=top_terms,
                primary_label=current_label,
                existing_alternatives=[]
            )
            return LLMEnhancedLabel(
                primary_label=current_label,
                alternative_labels=fallback_alternatives,
                description=f"Cluster characterized by: {', '.join(top_terms[:5])}",
                reasoning="Generated from top weighted terms (LLM not available)",
                confidence=0.5,
                source="fallback"
            )

        # Build and send prompt
        prompt = self._build_interpretation_prompt(
            top_terms=top_terms,
            term_weights=term_weights,
            sample_texts=sample_texts or [],
            current_label=current_label,
            document_count=document_count
        )

        response, source = self._generate_with_fallback(prompt)

        # Parse response
        if response:
            parsed = self._parse_llm_response(response)
            if parsed:
                primary_label = parsed.get("primary_label", current_label)
                alternatives = parsed.get("alternative_labels", [])[:3]

                # Ensure we have at least one alternative - generate from terms if needed
                if len(alternatives) < 1:
                    logger.debug(f"LLM provided no alternatives for '{primary_label}', generating from terms")
                    alternatives = self._generate_alternative_from_terms(
                        top_terms=top_terms,
                        primary_label=primary_label,
                        existing_alternatives=alternatives
                    )

                return LLMEnhancedLabel(
                    primary_label=primary_label,
                    alternative_labels=alternatives,
                    description=parsed.get("description", ""),
                    reasoning=parsed.get("reasoning", ""),
                    confidence=float(parsed.get("confidence", 0.8)),
                    source=source
                )

        # Fallback to term-based label with generated alternatives
        logger.info("LLM interpretation failed, using term-based fallback")
        fallback_alternatives = self._generate_alternative_from_terms(
            top_terms=top_terms,
            primary_label=current_label,
            existing_alternatives=[]
        )
        return LLMEnhancedLabel(
            primary_label=current_label,
            alternative_labels=fallback_alternatives,
            description=f"Cluster characterized by: {', '.join(top_terms[:5])}",
            reasoning="Fallback to term-based interpretation",
            confidence=0.5,
            source="fallback"
        )

    def enhance_all_clusters(
        self,
        cluster_summaries: Dict[str, Any],
        texts: List[str],
        cluster_assignments: List[int]
    ) -> Dict[str, LLMEnhancedLabel]:
        """
        Enhance all cluster summaries with LLM interpretation.

        Args:
            cluster_summaries: Dictionary of ClusterSummary objects
            texts: Original text documents
            cluster_assignments: Cluster assignment for each document

        Returns:
            Dictionary mapping cluster_id to LLMEnhancedLabel
        """
        enhanced_labels = {}

        # Group texts by cluster with their indices for sampling
        cluster_texts_with_indices = {}
        for idx, cluster_id in enumerate(cluster_assignments):
            cluster_key = f"CLUSTER_{cluster_id + 1:02d}"
            if cluster_key not in cluster_texts_with_indices:
                cluster_texts_with_indices[cluster_key] = []
            cluster_texts_with_indices[cluster_key].append((idx, texts[idx]))

        # Process each cluster
        for cluster_id, summary in cluster_summaries.items():
            # Handle both ClusterSummary objects and dicts
            if hasattr(summary, 'top_terms'):
                top_terms = summary.top_terms
                term_weights = summary.term_weights
                current_label = summary.label
                doc_count = summary.document_count
                representative_docs = getattr(summary, 'representative_docs', [])
            else:
                top_terms = summary.get('top_terms', [])
                term_weights = summary.get('term_weights', [])
                current_label = summary.get('label', '')
                doc_count = summary.get('document_count', 0)
                representative_docs = summary.get('representative_docs', [])

            # Get sample texts - prefer representative docs (closest to cluster center)
            sample_texts = []
            if representative_docs:
                # Use representative docs - these are selected by similarity to cluster center
                for doc in representative_docs[:8]:
                    if isinstance(doc, dict) and 'text' in doc:
                        sample_texts.append(doc['text'])
                    elif isinstance(doc, str):
                        sample_texts.append(doc)

            # If not enough representative docs, supplement with cluster texts
            if len(sample_texts) < 8:
                cluster_text_list = cluster_texts_with_indices.get(cluster_id, [])
                # Get diverse samples: pick from beginning, middle, and end of cluster
                if cluster_text_list:
                    n_texts = len(cluster_text_list)
                    # Sample indices spread across the cluster
                    sample_indices = self._get_diverse_sample_indices(n_texts, 8 - len(sample_texts))
                    for idx in sample_indices:
                        if idx < n_texts:
                            sample_texts.append(cluster_text_list[idx][1])

            enhanced = self.enhance_cluster_summary(
                top_terms=top_terms,
                term_weights=term_weights,
                current_label=current_label,
                document_count=doc_count,
                sample_texts=sample_texts
            )

            enhanced_labels[cluster_id] = enhanced
            logger.debug(f"Enhanced {cluster_id}: {enhanced.primary_label} (source: {enhanced.source})")

        return enhanced_labels

    def _get_diverse_sample_indices(self, n_total: int, n_samples: int) -> List[int]:
        """
        Get diverse sample indices spread across a collection.

        Instead of just taking the first N items, this spreads samples
        across the entire collection for better representation.
        """
        if n_total <= n_samples:
            return list(range(n_total))

        # Spread samples evenly across the collection
        step = n_total / (n_samples + 1)
        indices = []
        for i in range(1, n_samples + 1):
            idx = int(step * i)
            if idx < n_total and idx not in indices:
                indices.append(idx)

        # If we still need more, add from the beginning
        while len(indices) < n_samples and len(indices) < n_total:
            for i in range(n_total):
                if i not in indices:
                    indices.append(i)
                    break

        return indices[:n_samples]


def get_llm_interpreter() -> LLMClusterInterpreter:
    """
    Factory function to get a configured LLM interpreter.

    Returns:
        Configured LLMClusterInterpreter instance
    """
    return LLMClusterInterpreter()


# Convenience function for quick enhancement
def enhance_cluster_labels(
    cluster_report,
    texts: List[str],
    cluster_assignments: List[int]
) -> Dict[str, LLMEnhancedLabel]:
    """
    Convenience function to enhance cluster labels with LLM.

    Args:
        cluster_report: ClusterInterpretationReport from ClusterInterpreter
        texts: Original text documents
        cluster_assignments: Cluster assignments

    Returns:
        Dictionary mapping cluster_id to LLMEnhancedLabel
    """
    interpreter = get_llm_interpreter()
    return interpreter.enhance_all_clusters(
        cluster_summaries=cluster_report.summaries,
        texts=texts,
        cluster_assignments=cluster_assignments
    )
