"""
LLM-assisted cluster interpretation for improved human readability.

This module provides optional LLM-based refinement of cluster labels and descriptions,
using Mistral API with silent fallback to local quantized models.

Key features:
- Post-clustering label refinement using LLM
- Generation of alternative topic labels
- Detailed topic descriptions beyond simple labels
- Silent fallback to local quantized Mistral models if API fails
- Fully optional - graceful degradation to term-based labels

Configuration via environment variables:
    MISTRAL_API_KEY: API key for Mistral API (optional)
    MISTRAL_DEFAULT_MODEL: Model to use (default: open-mixtral-8x7b)
    LLM_INTERPRETATION_ENABLED: Enable/disable LLM interpretation (default: true)
    LLM_LOCAL_MODEL_PATH: Path to local quantized model (default: models/)

Usage:
    >>> from src.llm_interpretation import LLMClusterInterpreter
    >>> interpreter = LLMClusterInterpreter()
    >>> enhanced_summary = interpreter.enhance_cluster_summary(cluster_summary)
"""

import os
import json
import logging
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

logger = logging.getLogger(__name__)

# Try to load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    logger.debug("python-dotenv not installed, using system environment variables only")


@dataclass
class LLMEnhancedLabel:
    """
    Enhanced cluster label generated by LLM.

    Attributes:
        primary_label: The main refined label for the cluster
        alternative_labels: List of alternative label suggestions
        description: Detailed description of what the cluster represents
        reasoning: Brief explanation of why this label was chosen
        confidence: LLM's confidence in the interpretation (0-1)
        source: Whether generated by 'api', 'local', or 'fallback' (term-based)
    """
    primary_label: str
    alternative_labels: List[str] = field(default_factory=list)
    description: str = ""
    reasoning: str = ""
    confidence: float = 0.0
    source: str = "fallback"

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            'primary_label': self.primary_label,
            'alternative_labels': self.alternative_labels,
            'description': self.description,
            'reasoning': self.reasoning,
            'confidence': round(self.confidence, 3),
            'source': self.source
        }


class MistralAPIClient:
    """
    Client for Mistral API with error handling.
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        timeout: int = 30
    ):
        """
        Initialize Mistral API client.

        Args:
            api_key: Mistral API key (defaults to MISTRAL_API_KEY env var)
            model: Model to use (defaults to MISTRAL_DEFAULT_MODEL env var)
            timeout: Request timeout in seconds
        """
        self.api_key = api_key or os.getenv("MISTRAL_API_KEY")
        self.model = model or os.getenv("MISTRAL_DEFAULT_MODEL", "open-mixtral-8x7b")
        self.timeout = timeout
        self._client = None
        self._available = None

    def _init_client(self) -> bool:
        """Initialize the Mistral client lazily."""
        if self._available is not None:
            return self._available

        if not self.api_key:
            logger.info("No MISTRAL_API_KEY found, API client not available")
            self._available = False
            return False

        try:
            from mistralai import Mistral
            self._client = Mistral(api_key=self.api_key)
            self._available = True
            logger.info(f"Mistral API client initialized with model: {self.model}")
            return True
        except ImportError:
            logger.warning("mistralai package not installed, API client not available")
            self._available = False
            return False
        except Exception as e:
            logger.warning(f"Failed to initialize Mistral client: {e}")
            self._available = False
            return False

    @property
    def is_available(self) -> bool:
        """Check if the API client is available."""
        return self._init_client()

    def generate(self, prompt: str, system_prompt: Optional[str] = None) -> Optional[str]:
        """
        Generate a response from the Mistral API.

        Args:
            prompt: User prompt
            system_prompt: Optional system prompt

        Returns:
            Generated text or None if failed
        """
        if not self._init_client():
            return None

        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            response = self._client.chat.complete(
                model=self.model,
                messages=messages
            )

            if response and response.choices:
                return response.choices[0].message.content
            return None

        except Exception as e:
            logger.warning(f"Mistral API call failed: {e}")
            return None


class LocalMistralClient:
    """
    Client for local quantized Mistral models using llama-cpp-python.

    Supports GGUF format models for efficient CPU/GPU inference.
    """

    # Default model filenames to search for
    DEFAULT_MODEL_NAMES = [
        "mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf",
        "mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf",
        "mistral-7b-instruct-v0.2.Q4_K_M.gguf",
        "mistral-7b-instruct-v0.2.Q5_K_M.gguf",
        "open-mistral-7b.Q4_K_M.gguf",
        "open-mistral-7b.Q5_K_M.gguf",
    ]

    def __init__(
        self,
        model_path: Optional[str] = None,
        n_ctx: int = 4096,
        n_threads: Optional[int] = None,
        n_gpu_layers: int = 0
    ):
        """
        Initialize local Mistral client.

        Args:
            model_path: Path to GGUF model file or directory containing models
            n_ctx: Context window size
            n_threads: Number of CPU threads (None for auto)
            n_gpu_layers: Number of layers to offload to GPU (0 for CPU only)
        """
        self.model_path = model_path or os.getenv("LLM_LOCAL_MODEL_PATH", "models/")
        self.n_ctx = n_ctx
        self.n_threads = n_threads
        self.n_gpu_layers = n_gpu_layers
        self._model = None
        self._available = None
        self._resolved_path = None

    def _find_model_file(self) -> Optional[Path]:
        """Find a valid model file."""
        path = Path(self.model_path)

        # If it's a file, use it directly
        if path.is_file() and path.suffix == ".gguf":
            return path

        # If it's a directory, search for model files
        if path.is_dir():
            # First, check for any of the default model names
            for name in self.DEFAULT_MODEL_NAMES:
                model_file = path / name
                if model_file.exists():
                    return model_file

            # Then, look for any .gguf file
            gguf_files = list(path.glob("*.gguf"))
            if gguf_files:
                # Prefer mixtral over mistral-7b
                mixtral_files = [f for f in gguf_files if "mixtral" in f.name.lower()]
                if mixtral_files:
                    return mixtral_files[0]
                return gguf_files[0]

        return None

    def _init_model(self) -> bool:
        """Initialize the local model lazily."""
        if self._available is not None:
            return self._available

        try:
            from llama_cpp import Llama
        except ImportError:
            logger.info("llama-cpp-python not installed, local model not available")
            self._available = False
            return False

        model_file = self._find_model_file()
        if not model_file:
            logger.info(f"No GGUF model found in {self.model_path}")
            self._available = False
            return False

        try:
            self._resolved_path = model_file
            logger.info(f"Loading local model from {model_file}...")

            self._model = Llama(
                model_path=str(model_file),
                n_ctx=self.n_ctx,
                n_threads=self.n_threads,
                n_gpu_layers=self.n_gpu_layers,
                verbose=False
            )

            self._available = True
            logger.info(f"Local model loaded successfully: {model_file.name}")
            return True

        except Exception as e:
            logger.warning(f"Failed to load local model: {e}")
            self._available = False
            return False

    @property
    def is_available(self) -> bool:
        """Check if the local model is available."""
        return self._init_model()

    @property
    def model_name(self) -> str:
        """Get the name of the loaded model."""
        if self._resolved_path:
            return self._resolved_path.stem
        return "unknown"

    def generate(self, prompt: str, system_prompt: Optional[str] = None) -> Optional[str]:
        """
        Generate a response using the local model.

        Args:
            prompt: User prompt
            system_prompt: Optional system prompt

        Returns:
            Generated text or None if failed
        """
        if not self._init_model():
            return None

        try:
            # Format as Mistral instruct format
            if system_prompt:
                full_prompt = f"<s>[INST] {system_prompt}\n\n{prompt} [/INST]"
            else:
                full_prompt = f"<s>[INST] {prompt} [/INST]"

            response = self._model(
                full_prompt,
                max_tokens=1024,
                temperature=0.3,
                top_p=0.9,
                stop=["</s>", "[INST]"]
            )

            if response and "choices" in response:
                return response["choices"][0]["text"].strip()
            return None

        except Exception as e:
            logger.warning(f"Local model generation failed: {e}")
            return None


class LLMClusterInterpreter:
    """
    LLM-assisted cluster interpretation with API and local fallback.

    This class enhances cluster summaries with LLM-generated labels,
    alternative suggestions, and detailed descriptions. It gracefully
    falls back to local models if API is unavailable, and to term-based
    labels if both fail.

    Example:
        >>> interpreter = LLMClusterInterpreter()
        >>> enhanced = interpreter.enhance_cluster_summary(
        ...     cluster_summary=summary,
        ...     sample_texts=["example 1", "example 2"]
        ... )
        >>> print(enhanced.primary_label)
        >>> print(enhanced.description)
    """

    # System prompt for cluster interpretation
    SYSTEM_PROMPT = """You are an expert qualitative researcher specializing in thematic analysis and coding of open-ended survey responses.

Your task is to interpret clusters of similar survey responses and generate:
1. A clear, concise primary label (2-4 words) that captures the main theme
2. Alternative label suggestions (2-3 alternatives)
3. A detailed description (2-3 sentences) explaining what this cluster represents
4. Brief reasoning for your interpretation

Guidelines:
- Labels should be human-readable and meaningful to non-technical stakeholders
- Focus on the underlying theme or sentiment, not just the keywords
- Consider the context of survey responses
- Be objective and neutral in your interpretations
- Descriptions should explain what types of responses fall into this cluster"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        api_model: Optional[str] = None,
        local_model_path: Optional[str] = None,
        enabled: Optional[bool] = None
    ):
        """
        Initialize the LLM cluster interpreter.

        Args:
            api_key: Mistral API key (defaults to env var)
            api_model: API model name (defaults to env var)
            local_model_path: Path to local model (defaults to env var)
            enabled: Whether LLM interpretation is enabled (defaults to env var)
        """
        # Check if enabled
        if enabled is None:
            enabled_str = os.getenv("LLM_INTERPRETATION_ENABLED", "true").lower()
            enabled = enabled_str in ("true", "1", "yes", "on")
        self.enabled = enabled

        if not self.enabled:
            logger.info("LLM interpretation is disabled")
            self._api_client = None
            self._local_client = None
            return

        # Initialize clients
        self._api_client = MistralAPIClient(api_key=api_key, model=api_model)
        self._local_client = LocalMistralClient(model_path=local_model_path)

        # Log availability
        if self._api_client.is_available:
            logger.info(f"LLM interpretation using API: {self._api_client.model}")
        elif self._local_client.is_available:
            logger.info(f"LLM interpretation using local model: {self._local_client.model_name}")
        else:
            logger.warning("No LLM backend available, will use term-based fallback")

    @property
    def is_available(self) -> bool:
        """Check if any LLM backend is available."""
        if not self.enabled:
            return False
        return (
            (self._api_client and self._api_client.is_available) or
            (self._local_client and self._local_client.is_available)
        )

    @property
    def backend_type(self) -> str:
        """Get the type of backend being used."""
        if not self.enabled:
            return "disabled"
        if self._api_client and self._api_client.is_available:
            return "api"
        if self._local_client and self._local_client.is_available:
            return "local"
        return "fallback"

    def _build_interpretation_prompt(
        self,
        top_terms: List[str],
        term_weights: List[float],
        sample_texts: List[str],
        current_label: str,
        document_count: int
    ) -> str:
        """Build the prompt for cluster interpretation."""
        # Format top terms with weights
        terms_with_weights = []
        for term, weight in zip(top_terms[:10], term_weights[:10]):
            terms_with_weights.append(f"  - {term} (weight: {weight:.3f})")
        terms_str = "\n".join(terms_with_weights)

        # Format sample texts
        samples_str = ""
        if sample_texts:
            samples = []
            for i, text in enumerate(sample_texts[:5], 1):
                # Truncate long texts
                truncated = text[:300] + "..." if len(text) > 300 else text
                samples.append(f"  {i}. \"{truncated}\"")
            samples_str = "\n".join(samples)

        prompt = f"""Analyze this cluster of survey responses and provide interpretation.

CLUSTER INFORMATION:
- Current auto-generated label: "{current_label}"
- Number of responses: {document_count}

TOP KEYWORDS (by importance):
{terms_str}

SAMPLE RESPONSES FROM THIS CLUSTER:
{samples_str if samples_str else "  (No samples available)"}

Please respond in the following JSON format:
{{
    "primary_label": "Your 2-4 word label",
    "alternative_labels": ["Alt 1", "Alt 2", "Alt 3"],
    "description": "2-3 sentence description of what this cluster represents",
    "reasoning": "Brief explanation of why you chose this interpretation",
    "confidence": 0.85
}}

Respond ONLY with the JSON, no additional text."""

        return prompt

    def _parse_llm_response(self, response: str) -> Optional[Dict[str, Any]]:
        """Parse the LLM response into structured data."""
        if not response:
            return None

        try:
            # Try to extract JSON from the response
            # Sometimes LLMs add extra text before/after the JSON
            response = response.strip()

            # Find JSON block
            start_idx = response.find("{")
            end_idx = response.rfind("}") + 1

            if start_idx >= 0 and end_idx > start_idx:
                json_str = response[start_idx:end_idx]
                return json.loads(json_str)
            return None

        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse LLM response as JSON: {e}")
            return None

    def _generate_with_fallback(self, prompt: str) -> Tuple[Optional[str], str]:
        """
        Generate response with API-first, local-fallback strategy.

        Returns:
            Tuple of (response_text, source) where source is 'api', 'local', or 'none'
        """
        # Try API first
        if self._api_client and self._api_client.is_available:
            response = self._api_client.generate(prompt, self.SYSTEM_PROMPT)
            if response:
                return response, "api"
            logger.debug("API call failed, trying local model")

        # Fall back to local model
        if self._local_client and self._local_client.is_available:
            response = self._local_client.generate(prompt, self.SYSTEM_PROMPT)
            if response:
                return response, "local"
            logger.debug("Local model failed")

        return None, "none"

    def enhance_cluster_summary(
        self,
        top_terms: List[str],
        term_weights: List[float],
        current_label: str,
        document_count: int,
        sample_texts: Optional[List[str]] = None
    ) -> LLMEnhancedLabel:
        """
        Enhance a cluster summary with LLM-generated interpretation.

        Args:
            top_terms: List of top terms for the cluster
            term_weights: Weights for each top term
            current_label: Current auto-generated label
            document_count: Number of documents in cluster
            sample_texts: Optional sample texts from the cluster

        Returns:
            LLMEnhancedLabel with refined label and description
        """
        # Return fallback if LLM not enabled or available
        if not self.enabled or not self.is_available:
            return LLMEnhancedLabel(
                primary_label=current_label,
                alternative_labels=[],
                description=f"Cluster characterized by: {', '.join(top_terms[:5])}",
                reasoning="Generated from top weighted terms (LLM not available)",
                confidence=0.5,
                source="fallback"
            )

        # Build and send prompt
        prompt = self._build_interpretation_prompt(
            top_terms=top_terms,
            term_weights=term_weights,
            sample_texts=sample_texts or [],
            current_label=current_label,
            document_count=document_count
        )

        response, source = self._generate_with_fallback(prompt)

        # Parse response
        if response:
            parsed = self._parse_llm_response(response)
            if parsed:
                return LLMEnhancedLabel(
                    primary_label=parsed.get("primary_label", current_label),
                    alternative_labels=parsed.get("alternative_labels", [])[:3],
                    description=parsed.get("description", ""),
                    reasoning=parsed.get("reasoning", ""),
                    confidence=float(parsed.get("confidence", 0.8)),
                    source=source
                )

        # Fallback to term-based label
        logger.info("LLM interpretation failed, using term-based fallback")
        return LLMEnhancedLabel(
            primary_label=current_label,
            alternative_labels=[],
            description=f"Cluster characterized by: {', '.join(top_terms[:5])}",
            reasoning="Fallback to term-based interpretation",
            confidence=0.5,
            source="fallback"
        )

    def enhance_all_clusters(
        self,
        cluster_summaries: Dict[str, Any],
        texts: List[str],
        cluster_assignments: List[int]
    ) -> Dict[str, LLMEnhancedLabel]:
        """
        Enhance all cluster summaries with LLM interpretation.

        Args:
            cluster_summaries: Dictionary of ClusterSummary objects
            texts: Original text documents
            cluster_assignments: Cluster assignment for each document

        Returns:
            Dictionary mapping cluster_id to LLMEnhancedLabel
        """
        enhanced_labels = {}

        # Group texts by cluster for sampling
        cluster_texts = {}
        for idx, cluster_id in enumerate(cluster_assignments):
            cluster_key = f"CLUSTER_{cluster_id + 1:02d}"
            if cluster_key not in cluster_texts:
                cluster_texts[cluster_key] = []
            cluster_texts[cluster_key].append(texts[idx])

        # Process each cluster
        for cluster_id, summary in cluster_summaries.items():
            # Get sample texts for this cluster
            sample_texts = cluster_texts.get(cluster_id, [])[:5]

            # Handle both ClusterSummary objects and dicts
            if hasattr(summary, 'top_terms'):
                top_terms = summary.top_terms
                term_weights = summary.term_weights
                current_label = summary.label
                doc_count = summary.document_count
            else:
                top_terms = summary.get('top_terms', [])
                term_weights = summary.get('term_weights', [])
                current_label = summary.get('label', '')
                doc_count = summary.get('document_count', 0)

            enhanced = self.enhance_cluster_summary(
                top_terms=top_terms,
                term_weights=term_weights,
                current_label=current_label,
                document_count=doc_count,
                sample_texts=sample_texts
            )

            enhanced_labels[cluster_id] = enhanced
            logger.debug(f"Enhanced {cluster_id}: {enhanced.primary_label} (source: {enhanced.source})")

        return enhanced_labels


def get_llm_interpreter() -> LLMClusterInterpreter:
    """
    Factory function to get a configured LLM interpreter.

    Returns:
        Configured LLMClusterInterpreter instance
    """
    return LLMClusterInterpreter()


# Convenience function for quick enhancement
def enhance_cluster_labels(
    cluster_report,
    texts: List[str],
    cluster_assignments: List[int]
) -> Dict[str, LLMEnhancedLabel]:
    """
    Convenience function to enhance cluster labels with LLM.

    Args:
        cluster_report: ClusterInterpretationReport from ClusterInterpreter
        texts: Original text documents
        cluster_assignments: Cluster assignments

    Returns:
        Dictionary mapping cluster_id to LLMEnhancedLabel
    """
    interpreter = get_llm_interpreter()
    return interpreter.enhance_all_clusters(
        cluster_summaries=cluster_report.summaries,
        texts=texts,
        cluster_assignments=cluster_assignments
    )
