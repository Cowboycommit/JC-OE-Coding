# LLM Configuration for Cluster Interpretation
# Copy this file to .env and configure your settings

# =============================================================================
# MISTRAL API CONFIGURATION
# =============================================================================

# Your Mistral API key (get one at https://console.mistral.ai/)
# If not set, the system will fall back to local models
MISTRAL_API_KEY=

# Model to use for API calls
# Options: open-mixtral-8x7b (recommended), mistral-large-latest, mistral-medium-latest
MISTRAL_DEFAULT_MODEL=open-mixtral-8x7b

# =============================================================================
# LLM INTERPRETATION SETTINGS
# =============================================================================

# Enable/disable LLM-assisted interpretation
# Set to 'false' to use only term-based labels
LLM_INTERPRETATION_ENABLED=true

# Path to local quantized models (for API fallback)
# The system will search this directory for .gguf model files
LLM_LOCAL_MODEL_PATH=models/

# =============================================================================
# LOCAL MODEL CONFIGURATION (for llama-cpp-python)
# =============================================================================

# Number of CPU threads for local inference (leave empty for auto-detection)
# LLM_N_THREADS=

# Number of GPU layers to offload (0 for CPU-only, higher values use more GPU)
# Requires llama-cpp-python compiled with CUDA/Metal support
LLM_N_GPU_LAYERS=0

# Context window size for local models
LLM_CONTEXT_SIZE=4096

# =============================================================================
# NOTES
# =============================================================================
#
# Priority order for LLM interpretation:
# 1. Mistral API (if MISTRAL_API_KEY is set and valid)
# 2. Local quantized model (if .gguf file found in LLM_LOCAL_MODEL_PATH)
# 3. Term-based fallback (always available, no LLM needed)
#
# Recommended local models (download from HuggingFace):
# - mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf (best quality, ~26GB)
# - mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf (higher quality, ~32GB)
# - mistral-7b-instruct-v0.2.Q4_K_M.gguf (smaller, ~4GB)
# - mistral-7b-instruct-v0.2.Q5_K_M.gguf (smaller, higher quality, ~5GB)
#
# Run `python scripts/setup_local_models.py` to download local models
